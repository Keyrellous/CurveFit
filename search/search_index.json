{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Curve Fit! Background CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly. IHME COVID-19 Project For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting. Getting Started To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit pip install . or use make install . Maintainers Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu )","title":"Home"},{"location":"#welcome-to-curve-fit","text":"","title":"Welcome to Curve Fit!"},{"location":"#background","text":"CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly.","title":"Background"},{"location":"#ihme-covid-19-project","text":"For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting.","title":"IHME COVID-19 Project"},{"location":"#getting-started","text":"To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit pip install . or use make install .","title":"Getting Started"},{"location":"#maintainers","text":"Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu )","title":"Maintainers"},{"location":"code/","text":"Map of the Code We first start by walking through the core curve fitting model , and then the extensions that make it possible for CurveFit to be used for forecasting over time including pipelines and predictive validity . Core Model curevefit.core Here we will walk through how to use CurveModel . First, here is an example that you can copy and paste into your Python interpreter to run start to finish. import pandas as pd import numpy as np import matplotlib.pyplot as plt from curvefit.core.model import CurveModel from curvefit.core.functions import ln_gaussian_cdf np . random . seed ( 1234 ) # Create example data -- both death rate and log death rate df = pd . DataFrame () df [ 'time' ] = np . arange ( 100 ) df [ 'death_rate' ] = np . exp ( . 1 * ( df . time - 20 )) / ( 1 + np . exp ( . 1 * ( df . time - 20 ))) + \\ np . random . normal ( 0 , 0.1 , size = 100 ) . cumsum () df [ 'ln_death_rate' ] = np . log ( df [ 'death_rate' ]) df [ 'group' ] = 'all' df [ 'intercept' ] = 1.0 # Set up the CurveModel model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) # Fit the model to estimate parameters model . fit_params ( fe_init = [ 0 , 0 , 1. ], fe_gprior = [[ 0 , np . inf ], [ 0 , np . inf ], [ 1. , np . inf ]]) # Get predictions y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) # Plot results plt . plot ( df . time , y_pred , '-' ) plt . plot ( df . time , df . ln_death_rate , '.' ) You should see a plot that looks like this: Now we will walk through each of the steps from above and explain how to use them in detail. Setting Up a Model The code for the core curve fitting model is curvefit.core.model.CurveModel . To initialize a CurveModel , you need a pandas data frame and information about what type of model you want to fit. It needs to know which columns represent what and some model parameters. df (pd.DataFrame) : data frame with all available information for the model col_t (str) : the column that indicates the independent variable col_obs (str) : the column that indicates the dependent variable col_covs (list{list{str}}) : list of lists of strings that indicate the covariates to use for each covariate col_group (str) : the column that indicates the group variable (even if you only have one group you must pass a column that indicates group membership) param_names (list{str}) : names of the parameters for your specific functional form (more in functions ) link_fun (list{function}) : list of link functions for each of the parameters var_link_fun (list{function}) : list of functions for the variables including fixed and random effects First, we create sample data frame where time is the independent variable, death_rate is the dependent variable, and group is a variable indicating which group an observation belongs to. In this example, we want to fit to the log erf (also referred to as log Gaussian CDF) functional form (see functions ) with identity link functions for each parameter and identity variable link functions for each parameter. In this example, no parameters have covariates besides an intercept column of 1's. model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) Functions The curvefit package has some built-in functions for curves to fit. However, this list is not exhaustive, and you may pass any callable function that takes in t (an independent variable) and params (a list of parameters) to the function to the CurveModel class for the fun argument. What you pass in for param_names in the CurveModel needs to match what the fun callable expects. The available built-in functions in curvefit.core.functions are: The Error Function gaussian_cdf : Gaussian cumulative distribution function gaussian_pdf : Gaussian probability distribution function ln_gaussian_cdf : log Gaussian cumulative distribution function ln_gaussian_pdf : log Gaussian probability distribution function The Expit Function (inverse of the logit function) expit : expit function log_expit : log expit function Please see the functions for information about the parametrization of these functions and how they relate to COVID-19 modeling. Fitting a Model Once you have a model defined, the method fit_params fits the model. At minimum, the only information that model.fit_params needs is initial values for the fixed effects. But there are many optional arguments that you can pass to model.fit_params to inform the optimization process. Below we describe each of these optional arguments. The result of fit_params is stored in CurveModel.result and the parameter estimates in CurveModel.params . Gaussian Priors fe_gprior and re_gprior Each parameter may have an associated Gaussian prior. This is optional and can be passed in as a list of lists. This specification, referring to our example will put Gaussian priors with mean 0 and standard deviation 1. on the alpha parameter, mean 0 and standard deviation 1e-3 on the beta parameter and mean 5 and standard deviation 10. on the p parameter. model . fit_params ( fe_gprior = [[ 0 , 1. ], [ 0 , 1e-3 ], [ 5 , 10. ]]) Likewise, you may have random effects Gaussian priors using the argument re_gprior , which has the same shape as fe_gprior , but refers to the random effects. For the specifications of fixed and random effects, please see the methods . Constraints fe_bounds and re_bounds You can also include parameter constraints for each of the fixed effects and the random effects. They are included as a list of lists. This specification, referring to our example , will bound all of the fixed effects between 0 and 100. and the random effects between -1 and 1. model . fit_params ( fe_bounds = [[ 0. , 100. ], [ 0. , 100. ], [ 0. , 100. ]], re_bounds = [[ - 1. , 1. ], [ - 1. , 1. ], [ - 1. , 1. ]] ) If you do not want to include random effects, set the bounds to be exactly 0. Please see more information on constraints in the methods . Initialization The optimization routine will perform better with smart starting values for the parameters. Initial values for the fixed effects, fe_init , are required and is passed in as a numpy array of the same length as your parameters. The initial values for the random effects, re_init , are passed in as a numpy array ordered by the group name and parameters. For example, if you had two groups in the model, the following would initialize the fixed effects at 1., 1., 1., and the random effects at -0.5, -0.5, -0.5, for the first group and 0.5, 0.5, 0.5, for the second group. import numpy as np model . fit_params ( fe_init = np . array ([ 1. , 1. , 1. ]), re_init = np . array ([ - 0.5 , - 0.5 , - 0.5 , 0.5 , 0.5 , 0.5 ]) ) There is an optional flag, smart_initialize that if True will run a model individually for each group in the model and use their fixed effects estimates to inform the initial values for both fixed and random effects of the mixed model that you want to fit. Optimization The optimization uses scipy.optimize.minimize and the \"L-BFGS-B\" which has a list of options that you can pass to it. These keyword options can be passed to the minimize function using the options argument. For example, the following would perform a maximum of 500 iterations and require an objective function tolerance of 1e-10. model . fit_params ( options = { 'ftol' : 1e-10 , 'maxiter' : 500 } ) If you have indicated that you want the model to do smart initialization with smart_initialize = True , then you can optionally pass a dictionary of smart_init_options to override the options just for the group-specific initial fits. Otherwise it will use all of the options in both the group-specific and overall fits. Please see scipy.optimize.minimize for more options. Please see the methods for more information about the optimization procedure. Obtaining Predictions from a Model To obtain predictions from a model that has been fit, use the method CurveModel.predict . The predict function needs to know which values of the independent variable you want to predict for, which group you want to predict for, and optionally, which space you want to predict in. For example, you might want to predict in ln_gaussian_cdf space but make predictions in ln_gaussian_pdf space. This is only possible for functions that are related to one another (see the functions section). Continuing with our example , the following call would make predictions at all of the times in the original data frame, for group \"all\" . y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) Model Pipelines curvefit.pipelines To customize the modeling process for a specific problem, and integrate the core model with predictive validity and uncertainty, there is a class curvefit.pipelines._pipeline.ModelPipeline that sets up the structure. Each file in curvefit.piplelines subclasses this ModelPipeline to have different types of modeling processes. A ModelPipeline needs to get much of the same information that is passed to CurveModel . The additional arguments that it needs are predict_space (callable) : a curvefit.core.functions function that matches what space you want to do predictive validity in all_cov_names (list{str}) : a list of all the covariate names that will be used obs_se_func (callable) : in place of col_obs_se we now need to define a function that produces the standard error as a function of the independent variable The overall run() method that will be used in ModelPipeline does the following things: ModelPipeline.run_init_model() : runs aspects of the model that will not be re-run during predictive validity and/or stores information for use later ModelPipeline.run_predictive_validity() : runs predictive validity, described here ModelPipeline.fit_residuals() : fits residuals from predictive validity ModelPipeline.create_draws() : creates random realizations of the mean function that for the uncertainty intervals Each subclass of ModelPipeline has different requirements, each of which are described in their respective docstrings. Available classes and a brief description of what they do are below: BasicModel : Runs one model jointly with all groups. BasicModelWithInit : Runs all models separately to do a smart initialization of the fixed and random effects, and then runs a joint model with all groups. TightLooseModel : Runs four models with different combinations of settings (one setting should be \"tight\", meaning that it follows the prior closely and one \"loose\" meaning that it follows the prior less closely) and covariate models (can place the covariates on different parameters across models -- by default one is called the \"beta\" model and one is called the \"p\" model referring to which parameter has covariates). The \"tight\" and \"loose\" model predictions are blended within each covariate covariate model by using a convex combination of the predictions over time. Then the two covariate models are averaged together with pre-specified weights. APModel : Runs group-specific models and introduces a functional prior on the log of the alpha and beta parameters for the erf family of functions. PreConditionedAPModel : Runs like an APModel with the erf family but dynamically adjusts the bounds for the fixed effects of group-specific models based on preconditioning that flags groups that still have an exponential rise in the dependent variable with respect to the independent variable. Predictive Validity curvefit.pv","title":"Code"},{"location":"code/#map-of-the-code","text":"We first start by walking through the core curve fitting model , and then the extensions that make it possible for CurveFit to be used for forecasting over time including pipelines and predictive validity .","title":"Map of the Code"},{"location":"code/#core-model","text":"curevefit.core Here we will walk through how to use CurveModel . First, here is an example that you can copy and paste into your Python interpreter to run start to finish. import pandas as pd import numpy as np import matplotlib.pyplot as plt from curvefit.core.model import CurveModel from curvefit.core.functions import ln_gaussian_cdf np . random . seed ( 1234 ) # Create example data -- both death rate and log death rate df = pd . DataFrame () df [ 'time' ] = np . arange ( 100 ) df [ 'death_rate' ] = np . exp ( . 1 * ( df . time - 20 )) / ( 1 + np . exp ( . 1 * ( df . time - 20 ))) + \\ np . random . normal ( 0 , 0.1 , size = 100 ) . cumsum () df [ 'ln_death_rate' ] = np . log ( df [ 'death_rate' ]) df [ 'group' ] = 'all' df [ 'intercept' ] = 1.0 # Set up the CurveModel model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) # Fit the model to estimate parameters model . fit_params ( fe_init = [ 0 , 0 , 1. ], fe_gprior = [[ 0 , np . inf ], [ 0 , np . inf ], [ 1. , np . inf ]]) # Get predictions y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) # Plot results plt . plot ( df . time , y_pred , '-' ) plt . plot ( df . time , df . ln_death_rate , '.' ) You should see a plot that looks like this: Now we will walk through each of the steps from above and explain how to use them in detail.","title":"Core Model"},{"location":"code/#setting-up-a-model","text":"The code for the core curve fitting model is curvefit.core.model.CurveModel . To initialize a CurveModel , you need a pandas data frame and information about what type of model you want to fit. It needs to know which columns represent what and some model parameters. df (pd.DataFrame) : data frame with all available information for the model col_t (str) : the column that indicates the independent variable col_obs (str) : the column that indicates the dependent variable col_covs (list{list{str}}) : list of lists of strings that indicate the covariates to use for each covariate col_group (str) : the column that indicates the group variable (even if you only have one group you must pass a column that indicates group membership) param_names (list{str}) : names of the parameters for your specific functional form (more in functions ) link_fun (list{function}) : list of link functions for each of the parameters var_link_fun (list{function}) : list of functions for the variables including fixed and random effects First, we create sample data frame where time is the independent variable, death_rate is the dependent variable, and group is a variable indicating which group an observation belongs to. In this example, we want to fit to the log erf (also referred to as log Gaussian CDF) functional form (see functions ) with identity link functions for each parameter and identity variable link functions for each parameter. In this example, no parameters have covariates besides an intercept column of 1's. model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf )","title":"Setting Up a Model"},{"location":"code/#functions","text":"The curvefit package has some built-in functions for curves to fit. However, this list is not exhaustive, and you may pass any callable function that takes in t (an independent variable) and params (a list of parameters) to the function to the CurveModel class for the fun argument. What you pass in for param_names in the CurveModel needs to match what the fun callable expects. The available built-in functions in curvefit.core.functions are: The Error Function gaussian_cdf : Gaussian cumulative distribution function gaussian_pdf : Gaussian probability distribution function ln_gaussian_cdf : log Gaussian cumulative distribution function ln_gaussian_pdf : log Gaussian probability distribution function The Expit Function (inverse of the logit function) expit : expit function log_expit : log expit function Please see the functions for information about the parametrization of these functions and how they relate to COVID-19 modeling.","title":"Functions"},{"location":"code/#fitting-a-model","text":"Once you have a model defined, the method fit_params fits the model. At minimum, the only information that model.fit_params needs is initial values for the fixed effects. But there are many optional arguments that you can pass to model.fit_params to inform the optimization process. Below we describe each of these optional arguments. The result of fit_params is stored in CurveModel.result and the parameter estimates in CurveModel.params .","title":"Fitting a Model"},{"location":"code/#gaussian-priors","text":"fe_gprior and re_gprior Each parameter may have an associated Gaussian prior. This is optional and can be passed in as a list of lists. This specification, referring to our example will put Gaussian priors with mean 0 and standard deviation 1. on the alpha parameter, mean 0 and standard deviation 1e-3 on the beta parameter and mean 5 and standard deviation 10. on the p parameter. model . fit_params ( fe_gprior = [[ 0 , 1. ], [ 0 , 1e-3 ], [ 5 , 10. ]]) Likewise, you may have random effects Gaussian priors using the argument re_gprior , which has the same shape as fe_gprior , but refers to the random effects. For the specifications of fixed and random effects, please see the methods .","title":"Gaussian Priors"},{"location":"code/#constraints","text":"fe_bounds and re_bounds You can also include parameter constraints for each of the fixed effects and the random effects. They are included as a list of lists. This specification, referring to our example , will bound all of the fixed effects between 0 and 100. and the random effects between -1 and 1. model . fit_params ( fe_bounds = [[ 0. , 100. ], [ 0. , 100. ], [ 0. , 100. ]], re_bounds = [[ - 1. , 1. ], [ - 1. , 1. ], [ - 1. , 1. ]] ) If you do not want to include random effects, set the bounds to be exactly 0. Please see more information on constraints in the methods .","title":"Constraints"},{"location":"code/#initialization","text":"The optimization routine will perform better with smart starting values for the parameters. Initial values for the fixed effects, fe_init , are required and is passed in as a numpy array of the same length as your parameters. The initial values for the random effects, re_init , are passed in as a numpy array ordered by the group name and parameters. For example, if you had two groups in the model, the following would initialize the fixed effects at 1., 1., 1., and the random effects at -0.5, -0.5, -0.5, for the first group and 0.5, 0.5, 0.5, for the second group. import numpy as np model . fit_params ( fe_init = np . array ([ 1. , 1. , 1. ]), re_init = np . array ([ - 0.5 , - 0.5 , - 0.5 , 0.5 , 0.5 , 0.5 ]) ) There is an optional flag, smart_initialize that if True will run a model individually for each group in the model and use their fixed effects estimates to inform the initial values for both fixed and random effects of the mixed model that you want to fit.","title":"Initialization"},{"location":"code/#optimization","text":"The optimization uses scipy.optimize.minimize and the \"L-BFGS-B\" which has a list of options that you can pass to it. These keyword options can be passed to the minimize function using the options argument. For example, the following would perform a maximum of 500 iterations and require an objective function tolerance of 1e-10. model . fit_params ( options = { 'ftol' : 1e-10 , 'maxiter' : 500 } ) If you have indicated that you want the model to do smart initialization with smart_initialize = True , then you can optionally pass a dictionary of smart_init_options to override the options just for the group-specific initial fits. Otherwise it will use all of the options in both the group-specific and overall fits. Please see scipy.optimize.minimize for more options. Please see the methods for more information about the optimization procedure.","title":"Optimization"},{"location":"code/#obtaining-predictions-from-a-model","text":"To obtain predictions from a model that has been fit, use the method CurveModel.predict . The predict function needs to know which values of the independent variable you want to predict for, which group you want to predict for, and optionally, which space you want to predict in. For example, you might want to predict in ln_gaussian_cdf space but make predictions in ln_gaussian_pdf space. This is only possible for functions that are related to one another (see the functions section). Continuing with our example , the following call would make predictions at all of the times in the original data frame, for group \"all\" . y_pred = model . predict ( t = df . time , group_name = df . group . unique () )","title":"Obtaining Predictions from a Model"},{"location":"code/#model-pipelines","text":"curvefit.pipelines To customize the modeling process for a specific problem, and integrate the core model with predictive validity and uncertainty, there is a class curvefit.pipelines._pipeline.ModelPipeline that sets up the structure. Each file in curvefit.piplelines subclasses this ModelPipeline to have different types of modeling processes. A ModelPipeline needs to get much of the same information that is passed to CurveModel . The additional arguments that it needs are predict_space (callable) : a curvefit.core.functions function that matches what space you want to do predictive validity in all_cov_names (list{str}) : a list of all the covariate names that will be used obs_se_func (callable) : in place of col_obs_se we now need to define a function that produces the standard error as a function of the independent variable The overall run() method that will be used in ModelPipeline does the following things: ModelPipeline.run_init_model() : runs aspects of the model that will not be re-run during predictive validity and/or stores information for use later ModelPipeline.run_predictive_validity() : runs predictive validity, described here ModelPipeline.fit_residuals() : fits residuals from predictive validity ModelPipeline.create_draws() : creates random realizations of the mean function that for the uncertainty intervals Each subclass of ModelPipeline has different requirements, each of which are described in their respective docstrings. Available classes and a brief description of what they do are below: BasicModel : Runs one model jointly with all groups. BasicModelWithInit : Runs all models separately to do a smart initialization of the fixed and random effects, and then runs a joint model with all groups. TightLooseModel : Runs four models with different combinations of settings (one setting should be \"tight\", meaning that it follows the prior closely and one \"loose\" meaning that it follows the prior less closely) and covariate models (can place the covariates on different parameters across models -- by default one is called the \"beta\" model and one is called the \"p\" model referring to which parameter has covariates). The \"tight\" and \"loose\" model predictions are blended within each covariate covariate model by using a convex combination of the predictions over time. Then the two covariate models are averaged together with pre-specified weights. APModel : Runs group-specific models and introduces a functional prior on the log of the alpha and beta parameters for the erf family of functions. PreConditionedAPModel : Runs like an APModel with the erf family but dynamically adjusts the bounds for the fixed effects of group-specific models based on preconditioning that flags groups that still have an exponential rise in the dependent variable with respect to the independent variable.","title":"Model Pipelines"},{"location":"code/#predictive-validity","text":"curvefit.pv","title":"Predictive Validity"},{"location":"methods/","text":"Overview CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable. COVID-19 functional forms We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Error Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this. Statistical Model Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p . Constraints Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds. Optimization Procedure The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below. Solver We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task. Derivatives We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms. Uncertainty Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development. Predictive Validity-Based Uncertainty We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below. Model-Based Uncertainty We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Methods"},{"location":"methods/#overview","text":"CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable.","title":"Overview"},{"location":"methods/#covid-19-functional-forms","text":"We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Error Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this.","title":"COVID-19 functional forms"},{"location":"methods/#statistical-model","text":"Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p .","title":"Statistical Model"},{"location":"methods/#constraints","text":"Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds.","title":"Constraints"},{"location":"methods/#optimization-procedure","text":"The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below.","title":"Optimization Procedure"},{"location":"methods/#solver","text":"We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task.","title":"Solver"},{"location":"methods/#derivatives","text":"We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms.","title":"Derivatives"},{"location":"methods/#uncertainty","text":"Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development.","title":"Uncertainty"},{"location":"methods/#predictive-validity-based-uncertainty","text":"We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below.","title":"Predictive Validity-Based Uncertainty"},{"location":"methods/#model-based-uncertainty","text":"We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Model-Based Uncertainty"},{"location":"updates/","text":"","title":"Release Notes"},{"location":"extract_md/covariate_xam/","text":"Using Covariates Generalized Error Function The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate. Fixed Effects We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate. Random effects For this example the random effects are constrained to be zero. Social Distance For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three. Time Grid A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T . Measurement Values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import sys import pandas import numpy import scipy import pdb import sandbox sandbox . path () import curvefit from curvefit.core.model import CurveModel # # model for the mean of the data def generalized_error_function ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return 0.5 * p * ( 1.0 + scipy . special . erf ( alpha * ( t - beta ) ) ) # # link function used for beta def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # true value for fixed effects fe_true = numpy . array ( [ a_true , b_true , c_true , phi_true ] ) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_true = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ) : social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_true [ i ] = [ alpha_true , beta_true , p_true ] params_true = numpy . transpose ( params_true ) measurement_value = generalized_error_function ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = [ [ 'cov_one' ], [ 'cov_one' , 'social_distance' ], [ 'cov_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_error_function col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # fe_init = fe_true / 3.0 re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check result for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'covariate.py: OK' ) sys . exit ( 0 )","title":"covariate_xam"},{"location":"extract_md/covariate_xam/#using-covariates","text":"","title":"Using Covariates"},{"location":"extract_md/covariate_xam/#generalized-error-function","text":"The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate.","title":"Generalized Error Function"},{"location":"extract_md/covariate_xam/#fixed-effects","text":"We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate.","title":"Fixed Effects"},{"location":"extract_md/covariate_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/covariate_xam/#social-distance","text":"For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right.","title":"Social Distance"},{"location":"extract_md/covariate_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/covariate_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three.","title":"Problem Settings"},{"location":"extract_md/covariate_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T .","title":"Time Grid"},{"location":"extract_md/covariate_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement Values"},{"location":"extract_md/covariate_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import sys import pandas import numpy import scipy import pdb import sandbox sandbox . path () import curvefit from curvefit.core.model import CurveModel # # model for the mean of the data def generalized_error_function ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return 0.5 * p * ( 1.0 + scipy . special . erf ( alpha * ( t - beta ) ) ) # # link function used for beta def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # true value for fixed effects fe_true = numpy . array ( [ a_true , b_true , c_true , phi_true ] ) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_true = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ) : social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_true [ i ] = [ alpha_true , beta_true , p_true ] params_true = numpy . transpose ( params_true ) measurement_value = generalized_error_function ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = [ [ 'cov_one' ], [ 'cov_one' , 'social_distance' ], [ 'cov_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_error_function col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # fe_init = fe_true / 3.0 re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check result for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'covariate.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/effects2params/","text":"Map Vector of Fixed and Random Effects to Parameter Matrix Syntax params = curvefit.core.effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand=True ) group_sizes The observations (measurements) are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes. covs The value len(covs) is the number of parameters in the model. The value len(covs[k]) is the number of fixed effects corresponding to the k-th parameter. The vector covs[k][j] is the j-th covariate vector corresponding to the k-th parameter. The length of covs[k][j] is equal to the total number of observations. link_fun The value len(link_fun) is equal to the number of parameters and link_fun[k] is a function with one argument and one result that transforms the k-th parameter. var_link_fun The value len(var_link_fun) is equal to the number of fixed effects and link_fun[i] is a function with one argument and one result that transforms the i-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1]) fixed effects correspond to the second parameter and so on. expand If expand is True ( False ), create parameters for each observation (for each group of observations). x This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The j-th sub-vector corresponds to the j-th group of observations. params Let f_i be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the vector, with length equal to the number of fixed effects, v_{i,j} = V_i \\left( f_i + r_{i,j} \\right) where V_i is the function var_link_fun[i] . If expand is true (false) j indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a numpy array with row dimension equal to the number of parameters. If expand is true (false), its column column dimension equal to the number of observations (number of groups). The value params[k][j] is P_k \\left( \\sum_{i(k)} v_i c_{i,j} \\right) where P_k is the function link_fun[k] , i(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the i-th fixed effect and the j-th observation, if expand is true, or j-th group, if expand is false.","title":"effects2params"},{"location":"extract_md/effects2params/#map-vector-of-fixed-and-random-effects-to-parameter-matrix","text":"","title":"Map Vector of Fixed and Random Effects to Parameter Matrix"},{"location":"extract_md/effects2params/#syntax","text":"params = curvefit.core.effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand=True )","title":"Syntax"},{"location":"extract_md/effects2params/#group_sizes","text":"The observations (measurements) are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes.","title":"group_sizes"},{"location":"extract_md/effects2params/#covs","text":"The value len(covs) is the number of parameters in the model. The value len(covs[k]) is the number of fixed effects corresponding to the k-th parameter. The vector covs[k][j] is the j-th covariate vector corresponding to the k-th parameter. The length of covs[k][j] is equal to the total number of observations.","title":"covs"},{"location":"extract_md/effects2params/#link_fun","text":"The value len(link_fun) is equal to the number of parameters and link_fun[k] is a function with one argument and one result that transforms the k-th parameter.","title":"link_fun"},{"location":"extract_md/effects2params/#var_link_fun","text":"The value len(var_link_fun) is equal to the number of fixed effects and link_fun[i] is a function with one argument and one result that transforms the i-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1]) fixed effects correspond to the second parameter and so on.","title":"var_link_fun"},{"location":"extract_md/effects2params/#expand","text":"If expand is True ( False ), create parameters for each observation (for each group of observations).","title":"expand"},{"location":"extract_md/effects2params/#x","text":"This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The j-th sub-vector corresponds to the j-th group of observations.","title":"x"},{"location":"extract_md/effects2params/#params","text":"Let f_i be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the vector, with length equal to the number of fixed effects, v_{i,j} = V_i \\left( f_i + r_{i,j} \\right) where V_i is the function var_link_fun[i] . If expand is true (false) j indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a numpy array with row dimension equal to the number of parameters. If expand is true (false), its column column dimension equal to the number of observations (number of groups). The value params[k][j] is P_k \\left( \\sum_{i(k)} v_i c_{i,j} \\right) where P_k is the function link_fun[k] , i(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the i-th fixed effect and the j-th observation, if expand is true, or j-th group, if expand is false.","title":"params"},{"location":"extract_md/extract_md.py/","text":"Extracting Markdown Documentation From Source Code Syntax bin/extract_md.py extract_dir The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code. file_list The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from. extra_special_words The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below. Start Section The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot . mkdocs.yml For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:). End Section The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section. Spell Checking Special words can be added to the correct spelling list for a particular section as follows: {spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included. Code Blocks A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output. Indentation If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source.","title":"extract_md.py"},{"location":"extract_md/extract_md.py/#extracting-markdown-documentation-from-source-code","text":"","title":"Extracting Markdown Documentation From Source Code"},{"location":"extract_md/extract_md.py/#syntax","text":"bin/extract_md.py","title":"Syntax"},{"location":"extract_md/extract_md.py/#extract_dir","text":"The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code.","title":"extract_dir"},{"location":"extract_md/extract_md.py/#file_list","text":"The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from.","title":"file_list"},{"location":"extract_md/extract_md.py/#extra_special_words","text":"The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below.","title":"extra_special_words"},{"location":"extract_md/extract_md.py/#start-section","text":"The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot .","title":"Start Section"},{"location":"extract_md/extract_md.py/#mkdocsyml","text":"For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:).","title":"mkdocs.yml"},{"location":"extract_md/extract_md.py/#end-section","text":"The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section.","title":"End Section"},{"location":"extract_md/extract_md.py/#spell-checking","text":"Special words can be added to the correct spelling list for a particular section as follows: {spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included.","title":"Spell Checking"},{"location":"extract_md/extract_md.py/#code-blocks","text":"A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output.","title":"Code Blocks"},{"location":"extract_md/extract_md.py/#indentation","text":"If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source.","title":"Indentation"},{"location":"extract_md/get_cppad_py.py/","text":"Download and Install cppad_py Syntax `bin/get_cppad_py.py [test] [debug] [prefix] Command Line Arguments The order of the command line arguments test , debug , and prefix does not matter test If this argument is present it must be test=True . In this case cppad_py will be tested before the install. debug If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed. prefix If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"get_cppad_py.py"},{"location":"extract_md/get_cppad_py.py/#download-and-install-cppad_py","text":"","title":"Download and Install cppad_py"},{"location":"extract_md/get_cppad_py.py/#syntax","text":"`bin/get_cppad_py.py [test] [debug] [prefix]","title":"Syntax"},{"location":"extract_md/get_cppad_py.py/#command-line-arguments","text":"The order of the command line arguments test , debug , and prefix does not matter","title":"Command Line Arguments"},{"location":"extract_md/get_cppad_py.py/#test","text":"If this argument is present it must be test=True . In this case cppad_py will be tested before the install.","title":"test"},{"location":"extract_md/get_cppad_py.py/#debug","text":"If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed.","title":"debug"},{"location":"extract_md/get_cppad_py.py/#prefix","text":"If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"prefix"},{"location":"extract_md/get_started_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed Effects We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three. Random effects For this example the random effects are constrained to be zero. Covariates This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution Time Grid A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta . Measurement values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 # # f(t, alpha, beta, p) def generalized_logistic ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # params_true params_true = numpy . array ( [ alpha_true , beta_true , p_true ] ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = generalized_logistic ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data ) ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_params * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_logistic col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three inv_link_fun = [ log_fun , identity_fun , log_fun ] fe_init = numpy . zeros ( num_fe ) for i in range ( num_fe ) : fe_init [ i ] = inv_link_fun [ i ]( params_true [ i ]) / 3.0 # re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) params_estimate = curve_model . params fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check relation between fixed effects and parameters for i in range ( num_fe ) : rel_error = params_estimate [ i ] / link_fun [ i ]( fe_estimate [ i ] ) - 1.0 assert abs ( rel_error ) < 10.0 * numpy . finfo ( float ) . eps # # check optimal parameters for i in range ( num_params ) : rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'get_started.py: OK' ) sys . exit ( 0 )","title":"get_started_xam"},{"location":"extract_md/get_started_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/get_started_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/get_started_xam/#fixed-effects","text":"We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three.","title":"Fixed Effects"},{"location":"extract_md/get_started_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/get_started_xam/#covariates","text":"This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used.","title":"Covariates"},{"location":"extract_md/get_started_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/get_started_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution","title":"Problem Settings"},{"location":"extract_md/get_started_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta .","title":"Time Grid"},{"location":"extract_md/get_started_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/get_started_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 # # f(t, alpha, beta, p) def generalized_logistic ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # params_true params_true = numpy . array ( [ alpha_true , beta_true , p_true ] ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = generalized_logistic ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data ) ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_params * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_logistic col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three inv_link_fun = [ log_fun , identity_fun , log_fun ] fe_init = numpy . zeros ( num_fe ) for i in range ( num_fe ) : fe_init [ i ] = inv_link_fun [ i ]( params_true [ i ]) / 3.0 # re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) params_estimate = curve_model . params fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check relation between fixed effects and parameters for i in range ( num_fe ) : rel_error = params_estimate [ i ] / link_fun [ i ]( fe_estimate [ i ] ) - 1.0 assert abs ( rel_error ) < 10.0 * numpy . finfo ( float ) . eps # # check optimal parameters for i in range ( num_params ) : rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'get_started.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/objective_fun/","text":"Curve Fitting Objective Function of Fixed and Random Effects Syntax val = objective_fun( x, t, obs, obs_se, covs, group_sizes, fun, loss_fun, link_fun, var_link_fun, fe_gprior, re_gprior, param_gprior, ) : Notation num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = len(fe_gprior) is the number of fixed effects. *num_group~ = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs matrix containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array . x is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The j-th sub-vector corresponds to the j-th group of observations. t is a vector with length num_obs containing the value of the independent variable corresponding to each observation. obs is a vector with length num_obs containing the observations (i.e. measurements). obs_se is a vector with length num_obs containing the standard deviation for the corresponding observation. covs For k = 1, ... , num_param -1, the value len(covs[k]) is the number of fixed effects corresponding to the k-th parameter. The vector covs[k][j] has length num_obs and is the j-th covariate vector corresponding to the k-th parameter. group_sizes The observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs . model_fun This vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The residual vector has length num_obs and is given by residual = ( obs - model_fun ( t , params )) / obs_se loss_fun This scalar value function maps the residual vector to the corresponding contribution to the objective function. For example, a Gaussian likelihood corresponds to loss_fun ( residual ) = 0.5 * sum ( residual * residual ) link_fun, var_link_fun are used to compute params ; see Notation fe_gprior is an num_fe by two numpy array. The value fe_gprior[i][0] is the prior mean for the i-th fixed effect and fe_gprior[i][1] is its standard deviation. re_gprior is an num_fe by num_groups by two numpy array, re_gprior[i,j,0] ( re_gprior[i,j,1] ) is the mean (standard deviation) for the random effect corresponding to the i-th fixed effect and the j-th group. param_gprior is a list with three elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) The value param_gprior[1][0] ( param_gprior[1][1] ) is a numpy array corresponding to the mean (standard deviation) for range_gprior .","title":"objective_fun"},{"location":"extract_md/objective_fun/#curve-fitting-objective-function-of-fixed-and-random-effects","text":"","title":"Curve Fitting Objective Function of Fixed and Random Effects"},{"location":"extract_md/objective_fun/#syntax","text":"val = objective_fun( x, t, obs, obs_se, covs, group_sizes, fun, loss_fun, link_fun, var_link_fun, fe_gprior, re_gprior, param_gprior, ) :","title":"Syntax"},{"location":"extract_md/objective_fun/#notation","text":"num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = len(fe_gprior) is the number of fixed effects. *num_group~ = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs matrix containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array .","title":"Notation"},{"location":"extract_md/objective_fun/#x","text":"is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The j-th sub-vector corresponds to the j-th group of observations.","title":"x"},{"location":"extract_md/objective_fun/#t","text":"is a vector with length num_obs containing the value of the independent variable corresponding to each observation.","title":"t"},{"location":"extract_md/objective_fun/#obs","text":"is a vector with length num_obs containing the observations (i.e. measurements).","title":"obs"},{"location":"extract_md/objective_fun/#obs_se","text":"is a vector with length num_obs containing the standard deviation for the corresponding observation.","title":"obs_se"},{"location":"extract_md/objective_fun/#covs","text":"For k = 1, ... , num_param -1, the value len(covs[k]) is the number of fixed effects corresponding to the k-th parameter. The vector covs[k][j] has length num_obs and is the j-th covariate vector corresponding to the k-th parameter.","title":"covs"},{"location":"extract_md/objective_fun/#group_sizes","text":"The observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs .","title":"group_sizes"},{"location":"extract_md/objective_fun/#model_fun","text":"This vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The residual vector has length num_obs and is given by residual = ( obs - model_fun ( t , params )) / obs_se","title":"model_fun"},{"location":"extract_md/objective_fun/#loss_fun","text":"This scalar value function maps the residual vector to the corresponding contribution to the objective function. For example, a Gaussian likelihood corresponds to loss_fun ( residual ) = 0.5 * sum ( residual * residual )","title":"loss_fun"},{"location":"extract_md/objective_fun/#link_fun-var_link_fun","text":"are used to compute params ; see Notation","title":"link_fun, var_link_fun"},{"location":"extract_md/objective_fun/#fe_gprior","text":"is an num_fe by two numpy array. The value fe_gprior[i][0] is the prior mean for the i-th fixed effect and fe_gprior[i][1] is its standard deviation.","title":"fe_gprior"},{"location":"extract_md/objective_fun/#re_gprior","text":"is an num_fe by num_groups by two numpy array, re_gprior[i,j,0] ( re_gprior[i,j,1] ) is the mean (standard deviation) for the random effect corresponding to the i-th fixed effect and the j-th group.","title":"re_gprior"},{"location":"extract_md/objective_fun/#param_gprior","text":"is a list with three elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) The value param_gprior[1][0] ( param_gprior[1][1] ) is a numpy array corresponding to the mean (standard deviation) for range_gprior .","title":"param_gprior"},{"location":"extract_md/param_time_fun/","text":"Predefined Parametric Functions of Time head Syntax result = curvefit.core.functions.fun(t, params) t This is a list or one dimensional numpy.array . params This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2] fun The possible values for fun are listed in the subheadings below: expit This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] } ln_expit This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p ) gaussian_cdf This is the generalized Gaussian error function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right] ln_gaussian_cdf This is the log of the generalized Gaussian error function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) gaussian_pdf This is the derivative of the generalized Gaussian error function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) ln_gaussian_pdf This is the log of the derivative of the generalized Gaussian error function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) dgaussian_pdf This is the second derivative of the generalized Gaussian error function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) result The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ]) Example param_time_fun_xam","title":"param_time_fun"},{"location":"extract_md/param_time_fun/#predefined-parametric-functions-of-time","text":"","title":"Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun/#head-syntax","text":"result = curvefit.core.functions.fun(t, params)","title":"head Syntax"},{"location":"extract_md/param_time_fun/#t","text":"This is a list or one dimensional numpy.array .","title":"t"},{"location":"extract_md/param_time_fun/#params","text":"This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2]","title":"params"},{"location":"extract_md/param_time_fun/#fun","text":"The possible values for fun are listed in the subheadings below:","title":"fun"},{"location":"extract_md/param_time_fun/#expit","text":"This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] }","title":"expit"},{"location":"extract_md/param_time_fun/#ln_expit","text":"This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p )","title":"ln_expit"},{"location":"extract_md/param_time_fun/#gaussian_cdf","text":"This is the generalized Gaussian error function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right]","title":"gaussian_cdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_cdf","text":"This is the log of the generalized Gaussian error function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_cdf"},{"location":"extract_md/param_time_fun/#gaussian_pdf","text":"This is the derivative of the generalized Gaussian error function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"gaussian_pdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_pdf","text":"This is the log of the derivative of the generalized Gaussian error function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_pdf"},{"location":"extract_md/param_time_fun/#dgaussian_pdf","text":"This is the second derivative of the generalized Gaussian error function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"dgaussian_pdf"},{"location":"extract_md/param_time_fun/#result","text":"The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ])","title":"result"},{"location":"extract_md/param_time_fun/#example","text":"param_time_fun_xam","title":"Example"},{"location":"extract_md/param_time_fun_xam/","text":"Example and Test of Predefined Parametric Functions of Time Function Documentation param_time_fun Example Source Code import sys import numpy import scipy import sandbox sandbox . path () import curvefit # eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 # def eval_expit ( t , alpha , beta , p ) : return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # def eval_gaussian_cdf ( t , alpha , beta , p ) : z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z ) ) / 2.0 # # test values for t, alpha, beta, p t = numpy . array ( [ 5.0 , 10.0 ] ) beta = numpy . array ( [ 30.0 , 20.0 ] ) alpha = 2.0 / beta p = numpy . array ( [ 0.1 , 0.2 ] ) params = numpy . vstack ( ( alpha , beta , p ) ) # # check expit value = curvefit . core . functions . expit ( t , params ) check = eval_expit ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_expit value = curvefit . core . functions . ln_expit ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_cdf value = curvefit . core . functions . gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_gaussian_cdf value = curvefit . core . functions . ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_pdf step = sqrt_eps * beta value = curvefit . core . functions . gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check ln_gaussian_pdf value = curvefit . core . functions . ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check_dgaussian_pdf step = quad_eps * beta value = curvefit . core . functions . dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # print ( 'param_time_fun.py: OK' ) sys . exit ( 0 )","title":"param_time_fun_xam"},{"location":"extract_md/param_time_fun_xam/#example-and-test-of-predefined-parametric-functions-of-time","text":"","title":"Example and Test of Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun_xam/#function-documentation","text":"param_time_fun","title":"Function Documentation"},{"location":"extract_md/param_time_fun_xam/#example-source-code","text":"import sys import numpy import scipy import sandbox sandbox . path () import curvefit # eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 # def eval_expit ( t , alpha , beta , p ) : return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # def eval_gaussian_cdf ( t , alpha , beta , p ) : z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z ) ) / 2.0 # # test values for t, alpha, beta, p t = numpy . array ( [ 5.0 , 10.0 ] ) beta = numpy . array ( [ 30.0 , 20.0 ] ) alpha = 2.0 / beta p = numpy . array ( [ 0.1 , 0.2 ] ) params = numpy . vstack ( ( alpha , beta , p ) ) # # check expit value = curvefit . core . functions . expit ( t , params ) check = eval_expit ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_expit value = curvefit . core . functions . ln_expit ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_cdf value = curvefit . core . functions . gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_gaussian_cdf value = curvefit . core . functions . ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_pdf step = sqrt_eps * beta value = curvefit . core . functions . gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check ln_gaussian_pdf value = curvefit . core . functions . ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check_dgaussian_pdf step = quad_eps * beta value = curvefit . core . functions . dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # print ( 'param_time_fun.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/random_effect_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed and Random Effects We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\exp \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned} Covariates The constant one is the only covariate in this example. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import math n_time = 21 # number of time points used in the simulation n_group = 4 # number of groups rel_tol = 5e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , - 1.0 , + 1.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = [ math . log ( 2.0 ) / b_true [ 0 ], - 0.2 , - 0.1 , + 0.1 , + 0.2 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ math . log ( 0.1 ), - 0.3 , - 0.15 , + 0.15 , + 0.3 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero. Time Grid A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 . Measurement values We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise. Prior We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect. fe_gprior = [ [ a_true [ 0 ], a_true [ 0 ] / 100.0 ], [ b_true [ 0 ], b_true [ 0 ] / 100.0 ], [ phi_true [ 0 ], phi_true [ 0 ] / 100.0 ], ] Example Source Code # ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # # f(t, alpha, beta, p) def generalized_logistic ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for j in range ( 1 , n_group + 1 ) : group_j = 'group_' + str ( j ) alpha_j = math . exp ( a_true [ 0 ] + a_true [ j ]) beta_j = b_true [ 0 ] + b_true [ j ] p_j = math . exp ( phi_true [ 0 ] + phi_true [ j ]) y_j = generalized_logistic ( time_grid , [ alpha_j , beta_j , p_j ]) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_j ) data_group += n_time * [ group_j ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_fe * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_logistic col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three fe_init = numpy . array ( [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] ) / 3.0 re_init = numpy . zeros ( num_re ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe options = { 'disp' : 0 , 'maxiter' : 200 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , fe_gprior , options = options ) fe_estimate = curve_model . result . x [ 0 : num_fe ] re_estimate = curve_model . result . x [ num_fe :] . reshape ( n_group , num_fe ) # ------------------------------------------------------------------------- # check fixed effects fe_truth = [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_truth [ i ] - 1.0 assert abs ( rel_error ) < rel_tol for j in range ( n_group ) : re_truth = [ a_true [ j + 1 ], b_true [ j + 1 ], phi_true [ j + 1 ] ] for i in range ( num_fe ) : rel_err = re_estimate [ j , i ] / re_truth [ i ] assert abs ( rel_error ) < rel_tol # print ( 'random_effect.py: OK' ) sys . exit ( 0 )","title":"random_effect_xam"},{"location":"extract_md/random_effect_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/random_effect_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/random_effect_xam/#fixed-and-random-effects","text":"We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\exp \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned}","title":"Fixed and Random Effects"},{"location":"extract_md/random_effect_xam/#covariates","text":"The constant one is the only covariate in this example.","title":"Covariates"},{"location":"extract_md/random_effect_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/random_effect_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import math n_time = 21 # number of time points used in the simulation n_group = 4 # number of groups rel_tol = 5e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , - 1.0 , + 1.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = [ math . log ( 2.0 ) / b_true [ 0 ], - 0.2 , - 0.1 , + 0.1 , + 0.2 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ math . log ( 0.1 ), - 0.3 , - 0.15 , + 0.15 , + 0.3 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero.","title":"Problem Settings"},{"location":"extract_md/random_effect_xam/#time-grid","text":"A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 .","title":"Time Grid"},{"location":"extract_md/random_effect_xam/#measurement-values","text":"We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/random_effect_xam/#prior","text":"We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect. fe_gprior = [ [ a_true [ 0 ], a_true [ 0 ] / 100.0 ], [ b_true [ 0 ], b_true [ 0 ] / 100.0 ], [ phi_true [ 0 ], phi_true [ 0 ] / 100.0 ], ]","title":"Prior"},{"location":"extract_md/random_effect_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # # f(t, alpha, beta, p) def generalized_logistic ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for j in range ( 1 , n_group + 1 ) : group_j = 'group_' + str ( j ) alpha_j = math . exp ( a_true [ 0 ] + a_true [ j ]) beta_j = b_true [ 0 ] + b_true [ j ] p_j = math . exp ( phi_true [ 0 ] + phi_true [ j ]) y_j = generalized_logistic ( time_grid , [ alpha_j , beta_j , p_j ]) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_j ) data_group += n_time * [ group_j ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_fe * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = generalized_logistic col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three fe_init = numpy . array ( [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] ) / 3.0 re_init = numpy . zeros ( num_re ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe options = { 'disp' : 0 , 'maxiter' : 200 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , fe_gprior , options = options ) fe_estimate = curve_model . result . x [ 0 : num_fe ] re_estimate = curve_model . result . x [ num_fe :] . reshape ( n_group , num_fe ) # ------------------------------------------------------------------------- # check fixed effects fe_truth = [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_truth [ i ] - 1.0 assert abs ( rel_error ) < rel_tol for j in range ( n_group ) : re_truth = [ a_true [ j + 1 ], b_true [ j + 1 ], phi_true [ j + 1 ] ] for i in range ( num_fe ) : rel_err = re_estimate [ j , i ] / re_truth [ i ] assert abs ( rel_error ) < rel_tol # print ( 'random_effect.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/sizes_to_indices/","text":"Converting sizes to corresponding indices. Syntax indices = curvefit.sizes_to_indices(sizes) sizes The argument sizes is a one dimensional numpy.array of integers sizes. The value sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order. indices The return value indices is a list with numpy.array elements. The value indices[i] is a vector with length equal to sizes[i] . This vector indices[i] starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of list[i] are monotone and increase by one between elements. Example sizes_to_indices_xam","title":"sizes_to_indices"},{"location":"extract_md/sizes_to_indices/#converting-sizes-to-corresponding-indices","text":"","title":"Converting sizes to corresponding indices."},{"location":"extract_md/sizes_to_indices/#syntax","text":"indices = curvefit.sizes_to_indices(sizes)","title":"Syntax"},{"location":"extract_md/sizes_to_indices/#sizes","text":"The argument sizes is a one dimensional numpy.array of integers sizes. The value sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order.","title":"sizes"},{"location":"extract_md/sizes_to_indices/#indices","text":"The return value indices is a list with numpy.array elements. The value indices[i] is a vector with length equal to sizes[i] . This vector indices[i] starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of list[i] are monotone and increase by one between elements.","title":"indices"},{"location":"extract_md/sizes_to_indices/#example","text":"sizes_to_indices_xam","title":"Example"},{"location":"extract_md/sizes_to_indices_xam/","text":"Example and Test of sizes_to_indices Function Documentation size_to_indices Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # sizes = [ 2 , 4 , 3 ] indices = curvefit . core . utils . sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ]) ) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ]) ) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ]) ) print ( 'sizes_to_indices.py: OK' ) sys . exit ( 0 )","title":"sizes_to_indices_xam"},{"location":"extract_md/sizes_to_indices_xam/#example-and-test-of-sizes_to_indices","text":"","title":"Example and Test of sizes_to_indices"},{"location":"extract_md/sizes_to_indices_xam/#function-documentation","text":"size_to_indices","title":"Function Documentation"},{"location":"extract_md/sizes_to_indices_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # sizes = [ 2 , 4 , 3 ] indices = curvefit . core . utils . sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ]) ) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ]) ) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ]) ) print ( 'sizes_to_indices.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/split_by_group/","text":"Split the dataframe by the group definition. Syntax data = split_by_group(df, col_group) df Provided dataframe. col_group Column name in the dataframe contains group definition. data Dictionary with key as the group definition and value as the corresponding dataframe. Example","title":"split_by_group"},{"location":"extract_md/split_by_group/#split-the-dataframe-by-the-group-definition","text":"","title":"Split the dataframe by the group definition."},{"location":"extract_md/split_by_group/#syntax","text":"data = split_by_group(df, col_group)","title":"Syntax"},{"location":"extract_md/split_by_group/#df","text":"Provided dataframe.","title":"df"},{"location":"extract_md/split_by_group/#col_group","text":"Column name in the dataframe contains group definition.","title":"col_group"},{"location":"extract_md/split_by_group/#data","text":"Dictionary with key as the group definition and value as the corresponding dataframe.","title":"data"},{"location":"extract_md/split_by_group/#example","text":"","title":"Example"},{"location":"extract_md/unzip_x/","text":"Extract Fixed and Random Effects from Single Vector Form Syntax fe, re = curvefit.core.effects2params.unzip_x(x, num_groups, num_fe) num_groups is the number of data groups. num_fe is the number of fixed effects. x is a numpy vector with length equal to (num_groups + 1)*num_fe fe this return value is a numpy vector containing the fist num_fe elements of x . re this return value is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] Example unzip_x_xam","title":"unzip_x"},{"location":"extract_md/unzip_x/#extract-fixed-and-random-effects-from-single-vector-form","text":"","title":"Extract Fixed and Random Effects from Single Vector Form"},{"location":"extract_md/unzip_x/#syntax","text":"fe, re = curvefit.core.effects2params.unzip_x(x, num_groups, num_fe)","title":"Syntax"},{"location":"extract_md/unzip_x/#num_groups","text":"is the number of data groups.","title":"num_groups"},{"location":"extract_md/unzip_x/#num_fe","text":"is the number of fixed effects.","title":"num_fe"},{"location":"extract_md/unzip_x/#x","text":"is a numpy vector with length equal to (num_groups + 1)*num_fe","title":"x"},{"location":"extract_md/unzip_x/#fe","text":"this return value is a numpy vector containing the fist num_fe elements of x .","title":"fe"},{"location":"extract_md/unzip_x/#re","text":"this return value is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ]","title":"re"},{"location":"extract_md/unzip_x/#example","text":"unzip_x_xam","title":"Example"},{"location":"extract_md/unzip_x_xam/","text":"Example and Test of unzip_x Function Documentation unzip_x Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # num_groups = 2 num_fe = 3 x = numpy . array ( range ( ( num_groups + 1 ) * num_fe ) ) fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ] ) for i in range ( num_groups ) : assert all ( re [ i ,:] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] ) print ( 'unzip_x.py: OK' ) sys . exit ( 0 )","title":"unzip_x_xam"},{"location":"extract_md/unzip_x_xam/#example-and-test-of-unzip_x","text":"","title":"Example and Test of unzip_x"},{"location":"extract_md/unzip_x_xam/#function-documentation","text":"unzip_x","title":"Function Documentation"},{"location":"extract_md/unzip_x_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # num_groups = 2 num_fe = 3 x = numpy . array ( range ( ( num_groups + 1 ) * num_fe ) ) fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ] ) for i in range ( num_groups ) : assert all ( re [ i ,:] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] ) print ( 'unzip_x.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"}]}